{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "premium-uniform",
   "metadata": {},
   "source": [
    "# End-to-End Machine Learning Example: California Housing Prices\n",
    "\n",
    "Here we'll use housing price dataset from the 1990 CA census from StatLib repository which contains metric such as population, income, and housing price for each block group (the smallest geographical unit for which the census publishes sample data, typically 600 to 3,000 people). Our goal is for our ML model to accurately predict the median housing price in any district, given all other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-athletics",
   "metadata": {},
   "source": [
    "## Model Approach\n",
    "\n",
    "Since we are given _labeled_ training examples (accurate census sample data which gives the expected output- median housing price- for each set of features), we can conclude that this is definitely a **supervised learning** task. Since we are looking to predict a value, it is a regression task, but more specifically, a **multiple regression** proble, since we need to consider multiple features to make the output prediction (e.g. features like population, median income, etc.). It is also considered a _univariate regression_ problem since we only need to predict a single value for each district; conversely if we needed to predict multiple values, it would be a _multivariate regression_ problem. Finally, since data is not continuously streaming into the system and the dataset is small enough to fit in memory, **batch learning** is fine for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-yahoo",
   "metadata": {},
   "source": [
    "### Performance Measure\n",
    "\n",
    "Typical for regression problems, we will use the Root Mean Square Error (RMSE) measurement to give an idea of how much error the system is making with predictions at any given time. RMSE is calculated by:\n",
    "$$ RMSE(\\boldsymbol{X},h) = \\sqrt{ \\frac{1}{m} \\sum^{m}_{i=1} \\left (h(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right )^{2} } $$\n",
    "\n",
    "Where: \n",
    "* $m$ is the number of samples in the dataset being currently measured\n",
    "* $\\boldsymbol{x^{(i)}}$ is a vector of all feature values (excluding label, $y^{(i)}$) of the i-th instance in the dataset\n",
    "  + For instance, if a district in the dataset has a longitude location of -118.29deg, latitude of 33.91deg, population of 1416, and a median income of \\\\$38,372 - with the label/median house value of \\\\$156,400- then the vector and label would look like:\n",
    "  $$ \\boldsymbol{x^{(i)}} = \\begin{pmatrix}\n",
    "  -118.29 \\\\\n",
    "  33.91 \\\\\n",
    "  1416 \\\\\n",
    "  38372\n",
    "  \\end{pmatrix} $$\n",
    "  $$ y^{(1)} = 156400 $$\n",
    "* $\\boldsymbol{X}$ is the matrix containing all feature values (excluding labels) for all instances in the dataset, which with the above example values, looks like:\n",
    "$$ \\boldsymbol{X} = \\begin{bmatrix}\n",
    "     \\left ( \\boldsymbol{x^{(1)}} \\right )^{T} \\\\\n",
    "     \\left ( \\boldsymbol{x^{(2)}} \\right )^{T} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\left ( \\boldsymbol{x^{(N)}} \\right )^{T} \\\\\n",
    "   \\end{bmatrix}\n",
    "   = \\begin{pmatrix} -118.29 & 33.91 & 1416 & 38372 \\\\\n",
    "                      \\vdots & \\vdots & \\vdots & \\vdots \\end{pmatrix} $$\n",
    "* $h$ is the system's prediction function (aka _hypothesis_); it's the system's output given a feature vector $\\boldsymbol{x}^{(i)}$, $\\hat{y}^{(i)}=h(\\boldsymbol{x}^{(i)})$\n",
    "\n",
    "Once could use another performance function which measures deltas between the prediction vectors and target value vectors, called Mean Absolute Error (MAE):\n",
    "$$ MAE(\\boldsymbol{X},h) = \\frac{1}{m}\\sum^{m}_{i=1}\\left | h(\\boldsymbol{x}^{(i)} - y^{(i)}  \\right | $$\n",
    "\n",
    "These various distance measures are also called _norms_:\n",
    "* Computing RMSE corresponds to the _Euclidean norm_, or the $\\ell_{2}$ norm, denoted colloquially as $\\begin{Vmatrix} \\cdot \\end{Vmatrix}$ (or $\\begin{Vmatrix} \\cdot \\end{Vmatrix}_{2}$ more specifically)\n",
    "* Computing MAE correspongs to the _Manhattan norm_ (because it can measure the distance between two city points where you can only travel in orthogonal blocks), or the $\\ell_{1}$ norm, denoted $\\begin{Vmatrix} \\cdot \\end{Vmatrix}_{1}$\n",
    "\n",
    "In general, the $\\ell_{k}$ norm of a vector $\\boldsymbol{v}$ containing $n$ elements is defined as:\n",
    "$$ \\begin{Vmatrix} v \\end{Vmatrix}_{k} = \\left ( |v_{0}|^{k} + |v_{1}|^{k} + \\dotsb + |v_{n}|^{k} \\right )^{1/k} $$\n",
    "The higher the norm index, the more it focuses on large values and neglects small ones, hence why RMSE is more sensitive to outliers than MAE, however when outliers are exponentially rare, RMSE performs very well and is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-brick",
   "metadata": {},
   "source": [
    "## Dataset Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-ambassador",
   "metadata": {},
   "source": [
    "### Dataset Download\n",
    "\n",
    "Here we will download the comma-separated values (CSV) file that contains our housing dataset, and load it into memory using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "\n",
    "DL_FOLDER    = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL  = DL_FOLDER + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "# create function to easily download & extract housing dataset tarball\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "    \n",
    "fetch_housing_data() # download now\n",
    "housing = load_housing_data()\n",
    "housing.describe() # show a summary of the numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render plots within notebook itself\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-block",
   "metadata": {},
   "source": [
    "Note that in the above histogram plots, there are a couple important points:\n",
    "* `median_income` values were normalized to values between 0.5 and 15 (e.g. a value of 3 is equivalent to about \\\\$30k). This preprocessing is fine, and common in ML tasks.\n",
    "* `housing_median_age` and `median_house_value` values are capped, which may cause an issue since the house value is our target attribute (label), and you don't want the ML model to learn that prices never go above that limit.\n",
    "* Attributes have very different scales, which we'll need to tackle with feature scaling.\n",
    "* Many of the plots are _tail-heavy_ (the distribution of values is not symmetrical about the mean) which can be difficult for some ML algorithms to detect patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-taste",
   "metadata": {},
   "source": [
    "### Creating Test Set\n",
    "\n",
    "Creating a test set could be as simple as picking some random subset of the dataset (usually around 20%, or less with larger datasets) and set them aside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_idx  = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_idx      = shuffled_idx[:test_set_size]\n",
    "    train_idx     = shuffled_idx[test_set_size:]\n",
    "    return data.iloc[train_idx], data.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2)\n",
    "print(\"Training dataset size: %d\" % len(train_set))\n",
    "print(\"Testing dataset size: %d\" % len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-steal",
   "metadata": {},
   "source": [
    "While the above works, each time you run the code, a different test data set is generated; over time this means your model will see the whole dataset, which you want to avoid. One way to prevent this would be to set the random number generator's seed (e.x. `np.random.seed()`) before calling `np.random.permutation()`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
